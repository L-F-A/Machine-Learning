#The svm class uses the package CVXOPT, Python Software for Convex Optimization, that is not part of the standard python, 
#but is included in Anaconda for example. 

import numpy as np
import warnings
#Everything else is imported locally when needed

##########################################################################################################################

class LinReg:
##########################################################################################################################
#                                                       Linear regression                                                #
#                                                                                                                        #
#                           Louis-Francois Arsenault, Columbia University la2518@columbia.edu (2016)                     #
##########################################################################################################################
#                                                                                                                        #
#       INPUTS:                                                                                                          #
#                                                                                                                        #
#   X   is a ndarray representing a matrix with Nlearn lines with all known examples and the columns
#       are the dimension of one x
#   y	is a 1d array representing a NlearnX1 vector with values for each known examples
#   x   is a 1d array representing a column vector 

#Add comments above
#Verify multiple outputs

	def __init__(self):
		pass

	def train(self,X,y):
		#We treat the constant term as part of the coeffs. w and thus
		#need to add 1 at the end of every input
		OneVec = np.ones((X.shape[0],1))
		XX = np.concatenate((X, OneVec), axis=1)
		
		#How many intances, dimensions of the inputs and how many outputs
		SizeXX = XX.shape
		self.pdim = SizeXX[1]
		self.Nlearn = SizeXX[0]
		if y.ndim == 2:
			self.Nout = y.shape[1]
		else:
			self.Nout = 1

		if self.pdim <= self.Nlearn:
			A = np.dot(XX.transpose(),XX)
			self.w = np.linalg.solve(A,XX.transpose().dot(y))
			self.method = "pxp"
		else:
			A = np.dot(XX,XX.transpose())
			wp = np.linalg.solve(A,y)
			self.w = wp.dot(XX)
			self.method = "NxN"

	def query(self,Xt):
		OneVec = np.ones((Xt.shape[0],1))
                XX = np.concatenate((Xt, OneVec), axis=1)
                return XX.dot(self.w)

	def score(self,Xt,ytrue,metric='MAE'):
	#Calculate the mean absolute error MAE by default, mean square error MSE or correlation coeff CORR
		ypredic = self.query(Xt)
		if metric == 'MAE':
			return np.sum(np.absolute(ypredic-ytrue),axis=0)/len(ytrue)
		elif metric == 'MSE':
			ydiff = (ypredic-ytrue)
                        return np.sum(ydiff**2,axis=0)/len(ytrue)
##########################################################################################################################

class RidgeReg:
##########################################################################################################################
#                                     			Ridge regression	                                         #
#                                                                                                                        #
#                           Louis-Francois Arsenault, Columbia University la2518@columbia.edu (2016)                     #
##########################################################################################################################
#                                                                                                                        #
#       INPUTS:                                                                                                          #
#                                                                   

# Add comments above
                                                     #
	def __init__(self):
                pass
	def train(self,X,y,lam):
                #We treat the constant term as part of the coeffs. w and thus
                #need to add 1 at the end of every input
                OneVec = np.ones((X.shape[0],1))
                XX = np.concatenate((X, OneVec), axis=1)

		#How many intances, dimensions of the inputs and how many outputs
                SizeXX = XX.shape
                self.pdim = SizeXX[1]
                self.Nlearn = SizeXX[0]
		self.lam=lam

		if y.ndim == 2:
                        self.Nout = y.shape[1]
                else:
                        self.Nout = 1

                if self.pdim <= self.Nlearn:
                        Imat = np.identity(self.pdim)
                        Imat[self.pdim-1][self.pdim-1] = 0.
                        A = np.dot(XX.transpose(),XX) + lam*Imat
                        self.w = np.linalg.solve(A,XX.transpose().dot(y))
                        self.method = "pxp"
                else:
		#In the case where there are less examples N than features dimensions p, switch to a NxN approach, giving
		#the solution with the smallest l2 norm among the infinite possible solutions
                        Imat = np.identity(self.Nlearn)
                        Imat[self.Nlearn-1][self.Nlearn-1] = 0.
                        A = np.dot(XX,XX.transpose()) + lam*Imat
                        wp = np.linalg.solve(A,y)
                        self.w = wp.dot(XX)
                        self.method = "NxN"

        def query(self,Xt):
		OneVec = np.ones((Xt.shape[0],1))
                XX = np.concatenate((Xt, OneVec), axis=1)
                return XX.dot(self.w)

	def score(self,Xt,ytrue,metric='MAE'):
        #Calculate the mean absolute error MAE by default, mean square error MSE or correlation coeff CORR
                ypredic = self.query(Xt)
                if metric == 'MAE':
                        return np.sum(np.absolute(ypredic-ytrue),axis=0)/len(ytrue)
                elif metric == 'MSE':
                        ydiff = (ypredic-ytrue)
                        return np.sum(ydiff**2,axis=0)/len(ytrue)

##########################################################################################################################
def ActLearnVar(Xt,lam,sig2,XA,type='InputX'):
#For Active learning in the context of ridge regression, returns an array with the new variances when diffent xt are tested
#type can be 'InputX' the matrix X with features inputs or 'InputA' the matrix A = X^TX already
	#New testing x
	NXt=Xt.shape[0]
	OneVec = np.ones((NXt,1))
	XXt = np.concatenate((Xt, OneVec), axis=1)

	pdim=XXt.shape[1]

	#Data features
	if type is 'InputX':
		OneVec = np.ones((X.shape[0],1))
        	XX = np.concatenate((X, OneVec), axis=1)
		A=np.dot(XX.transpose(),XX)	

        Imat = np.identity(pdim)
        Imat[pdim-1][pdim-1] = 0.

	if NXt == 1:
		Ap=np.outer(XXt,XXt) + A
                b=np.linalg.solve(Ap,XXt)
                sig02=np.array(sig2+np.dot(XXt,b))
		id_pop=0
        else:
                sig02=1e18
		
                for r in range(NXt):
                	Aptemp=np.outer(XXt[r,:],XXt[r,:]) + A
                        b=np.linalg.solve(Aptemp,XXt[r,:])
                        sig02temp=sig2+np.dot(XXt[r,:],b)
			if sig02temp < sig02:
				sig02=sig02temp
				Ap=Aptemp
				id_pop=r 
		
	return id_pop,sig02,Ap
##########################################################################################################################

class LogisReg:
##########################################################################################################################
#                                     		     Logistic regression	                                         #
#                                                                                                                        #
#                           Louis-Francois Arsenault, Columbia University la2518@columbia.edu (2016)                     #
##########################################################################################################################
#                                                                                                                        #
#       INPUTS:                                                                                                          #
#                                                                                                                        #


#Add comments above
#Add second derivative -> Hessian. Done for L2 regularization below

	def __init__(self):
		#Logistic regression with no regularization
		pass
	def __logLike_logistic(self,w,XX,y):
                n = len(y)
                WdotX = XX.dot(w)
                l1 = -np.sum(np.log(1.+np.exp(WdotX)))/n
                l2 = np.sum(y.dot(WdotX))/n
                return -(l1+l2)

        def __logLike_logistic_der(self,w,XX,y):
		n = len(y)
                WdotX = XX.dot(w)
                p = 1./(1.+np.exp(-WdotX))
                dyp = y-p
                col = XX.shape[1]-1
                dl = np.zeros_like(w)
                for r in range(0,col+1):
                        dl[r] = -np.sum( dyp.dot(XX[:,r])  )/n
		return dl

	def train(self,X,y):
		from scipy.optimize import minimize
		OneVec = np.ones((X.shape[0],1))
                XX = np.concatenate((X, OneVec), axis=1)
		tupARG = (XX,y)
		w_0=np.zeros(XX.shape[1])
		res = minimize(self.__logLike_logistic,w_0,args=tupARG,method='BFGS', jac=self.__logLike_logistic_der,tol=1e-8)
		self.w = res.x
	
	def query(self,x):
		OneVec = np.ones((x.shape[0],1))
                XX = np.concatenate((x, OneVec), axis=1)
                WdotX = XX.dot(self.w)
                p = 1./(1.+np.exp(-WdotX))
                predic = np.rint(p)
                ProbPred = np.zeros((2,len(p)))
                ProbPred[0,0:len(p)] = p
                ProbPred[1,0:len(p)] = predic
                return ProbPred

	def score(self,X,ytrue):
	#Return the % of correct predictions
		Predic = self.query(X)
		ypredic = Predic[1,:]
		return 1-np.sum(np.absolute(ypredic-ytrue))/len(ytrue)
##########################################################################################################################

class LogisRegRegu:
#########################################################################################################################
#                                     Logistic regression with L2 or L1 regularization                                  #
#                                                                                                                       #
#                           Louis-Francois Arsenault, Columbia University la2518@columbia.edu (2016)                    #
#########################################################################################################################
#                                                                                                                       #
#       INPUTS:														#
#		type     : Regularization type. The choice are 'L2' (default) and 'L1'                                  #
#		solver   : Which solver: 'sp' uses scipy minimized, NEWTON-CG for L2 and POWELL for L1			#
#				         'gd' gradient descent								#
#				         'sg' stochastic gradient descent						#
#		eta      : The value to multiply the gradient in 'gd' or 'sg'						#
#		tol and  : Tolerance and relative tolerance for w in 'gd' or 'sg'. Will convergence if any 		#
#		tol_rel    respected											#
#		Nbatch   : When 'sg', the approximate size of the mini batchs						#
#		epochMax : When 'sg', maximum number of epochs to go							#
#															#
#	OUTPUTS:													#
#		self.w 	 : The vector w. w[-1] is the bias								#
#		self.solver												#
#		self.TYPE												#
#		self.Nlearn												#
#															#
#		COMMON TO 'gd' and 'sg':										#
#		self.eta 												#
#		self.tol												#
#		self.tol_rel												#
#															#
#		UNIQUE TO 'gd'												#
#		self.ite : NUmber of iteration that were necessary							#
#															#
#		UNIQUE TO 'sg'												#
#		self.Nbatch 												#
#		self.epochMax												#
#		self.epoch : How many epochs were necessary								#
#########################################################################################################################

        def __init__(self,type='L2',solver='sp',eta=None,tol=None,tol_rel=None,Nbatch=None,epochMax=None):
		self.solver=solver	
		if solver is 'sp':
                	#Reguralized logistic regression. The possible types are 'L2' (default) and 'L1'
                	self.TYPE = type
		elif solver is 'gd':
			self.TYPE='L2'
			self.eta=eta
			self.tol=tol
                        self.tol_rel=tol_rel
		elif solver is 'sg':#Solving using stochastic gradient descent with mini batch of size around Nbatch (no$self.type='L2'#Only L2 for the moment for stochastic gradient
			self.TYPE='L2'
			self.Nbatch=Nbatch
                        self.eta=eta
                        self.epochMax=epochMax
                        self.tol=tol
                        self.tol_rel=tol_rel
		else:
			raise ValueError('solver must be sp, gd or sd only ')

	def __MSE(self,w,XX,y):
		WdotX = XX.dot(w)
                p = 1./(1.+np.exp(-WdotX))
                return np.mean( (y-p)**2 )

	def __logLike_logistic_L(self,w,XX,y,lam):
                WdotX = XX.dot(w)
                l1 = -np.sum( np.log( 1.+np.exp(WdotX) ) )
                l2 = (y.dot(WdotX))
                if self.TYPE == 'L2':
                        l3 = -0.5*lam*np.sum(w[0:-1]**2)
                elif self.TYPE == 'L1':
                        l3 = -lam*np.sum(np.absolute(w[0:-1]))
                return -(l1+l2+l3)

        def __logLike_logistic_der_L2(self,w,XX,y,lam):
                WdotX = XX.dot(w)
                p = 1./(1.+np.exp(-WdotX))
                dyp = y-p
		dl=-dyp.dot(XX) + lam*w
		dl[-1]=dl[-1]-lam*w[-1]
                return dl

	def __logLike_logistic_der_L2_forSG(self,w,XX,y,lam):
                WdotX = XX.dot(w)
                p = 1./(1.+np.exp(-WdotX))
                dyp = y-p
		dl=-dyp*XX + lam*w
                dl[-1]=dl[-1]-lam*w[-1]
                return dl

	def __logLike_logistic_der2_L2(self,w,XX,y,lam):
                WdotX = XX.dot(w)
                p = 1./(1.+np.exp(-WdotX))
		#Can I avoid creating the identity matrix every time? Pass as parameter?
		Imat = np.identity(XX.shape[1])
                Imat[XX.shape[1]-1][XX.shape[1]-1] = 0
		Qmat = np.diag(p*(1-p)) 
		return XX.transpose().dot( Qmat.dot(XX) )+lam*Imat

	def train(self,X,y,lam):
		
		OneVec = np.ones((X.shape[0],1))
                XX = np.concatenate((X, OneVec), axis=1)
		w_0 = np.zeros(XX.shape[1])
		self.Nlearn=XX.shape[0]
		
		if self.solver is 'sp':
		#Using Scipy minimizers
			from scipy.optimize import minimize
			tupARG = (XX,y,lam)
                	if self.TYPE == 'L2':
                        	res = minimize(self.__logLike_logistic_L,w_0,args=tupARG,method='Newton-CG',jac=self.__logLike_logistic_der_L2,tol=1e-8,options={'xtol': 1e-08})
                        	self.w = res.x
                	elif self.TYPE == 'L1':
                        	res = minimize(self.__logLike_logistic_L,w_0,args=tupARG,method='Powell',tol=1e-8)
                        	self.w = res.x
				self.w[np.abs(self.w)<=1e-10]=0.

		elif self.solver is 'gd':
		#Using homemade gradient descent
			from NumMethods import GradDescSteep
			w_0,ite=GradDescSteep(self.__logLike_logistic_der_L2,w_0,self.eta,self.tol,self.tol_rel,args=(XX,y,lam))
			self.w=w_0
			self.ite=ite

		else:
		#Using stichastic gradient descent
			idx=range(self.Nlearn)
                        when_break=0
			eta=self.eta
                        ite=0
			t=0.
			err=self.__MSE(w_0,XX,y)
			ERR=[]
			err_rel=10.
			ep=0
                        #for ep in range(self.epochMax):
			while (err>self.tol):# or (err_rel>sel.tol_rel):

                                np.random.shuffle(idx)#Create the order for one epoch
				if self.Nbatch==1:
					sp=idx
				else:
                                	sp=np.array_split(idx,np.floor(self.Nlearn/float(self.Nbatch)))

                                for r_sp in range(len(sp)):

                                        idData=sp[r_sp]
                                        #w_00=w_0.copy()

                                        #Update of the vector w
					if self.Nbatch==1:
						w_0=w_0-eta*self.__logLike_logistic_der_L2_forSG(w_0,XX[idData,:],y[idData],lam)
					else:
						n=len(idData)
                                        	w_0=w_0-eta/n*self.__logLike_logistic_der_L2(w_0,XX[idData,:],y[idData],lam)
					#if t==5:
					#	LF0=self.__logLike_logistic_L(w_0,XX,y,lam)
					#err=np.abs(w_0-w_00)
                                        #err_rel=err/np.abs(w_00+np.finfo(float).eps)
                                        #if (np.max(err)<self.tol) or (np.max(err_rel)<self.tol_rel):
                                        #        self.w=w_0
                                        #        when_break=1
					#	self.epoch=ep+1
                                        #        break
					#elif (r_sp==len(sp)-1) and (ep==self.epochMax-1):
                                        #        mess='No convergenge after '+str(self.epochMax)+' epochs'
                                        #        warnings.warn(mess)
					#	self.w=w_0
                                        #        when_break=1
					#	break
					t+=1.
					#eta=eta/(1.+self.eta*lam*t)
				#err=self.__logLike_logistic_L(w_0,XX,y,lam)
				err=self.__MSE(w_0,XX,y)
				ERR.append(err)
				#err_tol=np.abs(err-LF0)/np.abs(LF0)
				LF0=err
				ep+=1
				eta=0.5*eta
				if ep==self.epochMax:
					mess='No convergenge after '+str(self.epochMax)+' epochs'
                                        warnings.warn(mess)
					break
			self.w=w_0
			self.epoch=ep
			self.ERR=ERR				
                                #if when_break==1:
                                #        break
				#if ep%2==0:
                               	#	eta=0.5*eta

        def query(self,x,threshold=0.5):
		OneVec = np.ones((x.shape[0],1))
                XX = np.concatenate((x, OneVec), axis=1)
		WdotX = XX.dot(self.w)
                p = 1./(1.+np.exp(-WdotX))
		predic = np.zeros(len(p))
		predic[p>threshold] = 1.
		#predic = np.rint(p)
		ProbPred = np.zeros((2,len(p)))
		ProbPred[0,:] = p
		ProbPred[1,:] = predic
                return ProbPred

	def score(self,X,ytrue):
        #Return the % of correct predictions
                Predic = self.query(X)
                ypredic = Predic[1,:]
                return 1-np.sum(np.absolute(ypredic-ytrue))/len(ytrue)
##########################################################################################################################

class SVMClass:
##########################################################################################################################
#        				     Linear only support vector machine                                          #
#                                                                                                                        #
#                           Louis-Francois Arsenault, Columbia University la2518@columbia.edu (2016)                     #
##########################################################################################################################
#															 #
#	INPUTS:														 #
#															 #
#	  typeL : 'norm2' (default) is the standard svm using ||w||_2^2 while 'norm1' is a 1-norm svm using ||w||_1 	 #
#		   rather with objective function as: ttp://papers.nips.cc/paper/2450-1-norm-support-vector-machines.pdf #
#															 #
#	  X 	:  A ndarray array([x_11,x_12,...,x_1p],[x_21,x_22,...,x_2p],...,[x_Nlearn1,x_Nlearn2,...,x_Nlearnp])    #
#		   representing a matrix with Nlearn lines with all known examples of the training set and the columns 	 #
#		   are the dimensions p of one x.									 #
#															 #
#   	  y 	:  A 1d array numpy.array([y_1,y_2,...,y_Nlearn]) representing a vector of size Nlearn with values for   #
#		   each examples of the training set in the form -1 and 1.						 #
#															 #
#	  C	:  Value of the slack variable to be used. The default value is set to 0, meaning a separable problem	 #
#															 #
#	  tolsvm : In the 2-norm case with non-zero C, what tolerance to define below which we have a support vector. 	 #
#		   The final tolerance is tolsvm = tolsvm*C.								 #
#															 #
#	OUTPUTS:													 #
#															 #
#	  self.Nlearn : Number of instances in training									 #
#															 #
#	  self.pdim : How many dimensions for the linear model given by vector w; equal to p+1 (constant term included)	 #
#															 #
#	  self.w : This is the linear model. A vector with p+1 elements. A prediction is sign( w[0:-1]^T*Xt + w[-1] ).	 #
#															 #
#	  In the 2-norm case with non-zero C:										 #
#															 #
#	  self.alpha : The alpha vector for the dual model								 #
#															 #
#	  self.indsv : The indices of the support vectors 								 #
##########################################################################################################################

        def __init__(self,typeL='norm2'):
                self.typeL = typeL
        def train(self,X,y,C=0,tolsvm=1e-5):
		if self.typeL is 'norm2': #2-norm svm
		#The 2-norm svm is the standard approach where the min. approach has the ||w||_2^2 term
			from NumMethods import quadprog 
			if C==0:
				#Solving the linearly separable problem with quad. prog.
				#We treat the constant term as part of the coeffs. w and thus
                        	#need to add 1 at the end of every input
                        	OneVec = np.ones((X.shape[0],1))
                        	XX = np.concatenate((X, OneVec), axis=1)

				#Number of instances in the training set as well as the dimension of final w
                        	SizeXX = XX.shape
                        	pdim = SizeXX[1]
                        	Nlearn = SizeXX[0]
                        	self.Nlearn = Nlearn
                        	self.pdim = pdim
                        	
				#Building the matrices for the quadratic prog. problem
				H1 = np.identity(pdim)
                        	H1[pdim-1,pdim-1] = 0.
                        	bineq1 = -np.ones((Nlearn,1))
                        	Aineq1 = -np.diag(y).dot(XX)
				q1 = np.zeros((1,pdim))[0]

				#Solve the quad. prog.
				self.w = quadprog(H1,q1,Aineq=Aineq1,bineq=bineq1)
				self.w.shape = (self.pdim,)
			else:
				#Solving the dual with quad. prog.
                                #The constant term is included in w at the end as the last value

				#Tolerance for the choice of support vectors
				self.tolsvm=tolsvm*C
				#Number of instances in the training set as well as the dimension of final w				
				SizeX = X.shape
                                pdim = SizeX[1]
                                Nlearn = SizeX[0]
                                self.Nlearn = Nlearn
                                self.pdim = pdim+1
				
				#Building the matrices for the quadratic prog. problem
				Hinter = np.dot(X,X.transpose())
				H1 = np.diag(y).dot( Hinter.dot( np.diag(y) ) )
				q1 = -np.ones(Nlearn)
				Aeq1 = y.copy()
				beq1 = np.array([0.])
				lb1 = np.zeros(Nlearn)
				ub1 = C*np.ones(Nlearn)

				#Solve the quad. prog.
				alpha=quadprog(H1,q1,Aeq=Aeq1,beq=beq1,lb=lb1,ub=ub1)
				self.alpha = alpha.transpose()[0]

				#Find the support vectors and only use them
				indsv = np.where((alpha > self.tolsvm) & (alpha < (C-self.tolsvm)))[0].astype(int)
                                self.indsv = indsv
				
				#The linear model is completely specified by the vector w
				self.w = np.zeros(pdim+1)
				self.w[0:-1] = self.alpha[indsv].dot(np.diag(y[indsv]).dot(X[indsv]))
				#The constant term obtained by the value for each support vector and then average them
				bias = np.mean(y[indsv]- X[indsv,:].dot(self.w[0:-1]))
				self.w[-1] = bias

		elif self.typeL is 'norm1': #1-norm svm
				#The 1-norm svm uses ||w||_1 rather than ||w||_2^2
				#For details, see http://papers.nips.cc/paper/2450-1-norm-support-vector-machines.pdf 
				from cvxopt.modeling import variable as cvxvar, op, sum as cvxsum, max as cvxmax
				from cvxopt.solvers import options as cvxopt
                                from cvxopt import matrix as cvxmat
				cvxopt['show_progress'] = False

				#Number of instances in the training set as well as the dimension of final w
				self.Nlearn = X.shape[0]
				self.pdim = X.shape[1]+1

				#Necessary matrices for the unconstrained problem
				AL1 = np.diag(y).dot(X)
				BL1 = cvxmat(C*y)
                                ALL1 = cvxmat(C*AL1)

				#The variables to be found
                                WW = cvxvar(ALL1.size[1],'WW')
				bb = cvxvar(1,'bb')

				#Calling the solver
                                op( cvxsum(abs(WW)) + cvxsum(cvxmax(0,1-(ALL1*WW + BL1*bb)))).solve()
				self.w = np.zeros(self.pdim)
				self.w[0:-1] = np.array(WW.value).reshape((self.pdim-1,))
				self.w[-1] = np.array(bb.value).reshape((1,))

	def query(self,Xt):
		#Prediction for matrix Xt
		OneVec = np.ones((Xt.shape[0],1))
                XXt = np.concatenate((Xt, OneVec), axis=1)
		return np.sign(XXt.dot(self.w))

	def score(self,X,ytrue):
	#Return the % of correct predictions
		ypredic = self.query(X)
                return 1-0.5*np.sum(np.absolute(ypredic-ytrue))/len(ytrue)
##########################################################################################################################

























#Solving the unconstrained problem formulation with C
#w0 = -1. + 2.*np.random.rand(pdim)
#tupARG=(C,Aineq1,bineq1,H1)
#res = minimize(self.__L,w0,args=tupARG,method='Nelder-Mead',tol=1e-11)
#self.w = res.x


#def __L(self,w,C,Aeq,beq,H1):
 #               D = beq-Aeq.dot(w)
  #              DD = np.concatenate((np.zeros((self.Nlearn,1)),D), axis=1)
   #             M = DD.max(axis=1)
    #            #lam = 1./self.Nlearn/C
     #           return 0.5*w.dot(H1.dot(w)) + C*np.sum(M)



#def ProposeW0(self,X,y,lam,wmin,wmax,iter):
 #               pdim = X.shape[1]+1
  #              w0 = np.zeros((1,pdim))
   #             w0_min = w0
    #            OneVec = np.ones((X.shape[0],1))
     #           XX = np.concatenate((X, OneVec), axis=1)
      #          tupARG = (XX,y,lam)
       #         if self.TYPE == 'L2':
        #                res = minimize(self.__logLike_logistic_L,w0,args=tupARG,method='BFGS',jac=self.__logLike_logistic_der_L2,tol=1e-4)
         #               funcVal = res.fun
          #              funcValMin = funcVal
           #     elif self.TYPE == 'L1':
            #            res = minimize(self.__logLike_logistic_L,w0,args=tupARG,method='Powell',tol=1e-3)
             #           funcVal = res.fun
              #          funcValMin = funcVal
               # for riter in range(0,iter):
                #        w0New = wmin + (wmax-wmin)*np.random.rand(1,pdim)
                 #       if self.TYPE == 'L2':
                  #              res = minimize(self.__logLike_logistic_L,w0New,args=tupARG,method='BFGS',jac=self.__logLike_logistic_der_L2,tol=1e-4)
                   #             funcVal = res.fun
                    #            if np.absolute(funcVal) < np.absolute(funcValMin):
                     #                   funcValMin = funcVal
                      #                  w0_min = w0New
                       # elif self.TYPE == 'L1':
                        #        res = minimize(self.__logLike_logistic_L,w0New,args=tupARG,method='Powell',tol=1e-3)
                         #       funcVal = res.fun
                          #      if np.absolute(funcVal) < np.absolute(funcValMin):
                           #             funcValMin = funcVal
                            #            w0_min = w0New
               # return w0_min
